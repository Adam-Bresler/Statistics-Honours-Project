<<<<<<< Updated upstream
setwd("~/GitHub/Statistics-Honours-Project/Data")
ata <- read.csv("rolling_average_all.csv")
=======
w <- c(0, -runif(1 , -1, 1), 1, -runif(1 , -1, 1))
Y <- sign(X %*% w)
three_d[j, i] <- system.time(PLA3(X1, X2, X3, Y))[[1]]
}
}
mean_time_3d <- colMeans(three_d)
mean_time_3d
plot(mean_time_2d~N, type = "l")
lines(mean_time_3d)
lines(mean_time_3d, N)
plot(mean_time_2d~N, type = "l")
lines(mean_time_3d, N)
plot(mean_time_2d~N, type = "l", col = "blue")
lines(N, mean_time_3d, col = "red")
plot(mean_time_2d~N, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.05))
lines(N, mean_time_3d, col = "red")
plot(mean_time_2d~N, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.05))
lines(N, mean_time_3d, col = "red")
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"))
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"), lty = 1)
plot(mean_time_2d ~ N, pch = 16, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.05))
plot(mean_time_2d ~ N, pch = 16, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.05))
lines(N, mean_time_3d, col = "red")
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"), lty = 1)
PLA2 = function(X1, X2, Y)
{
# Create a design matrix and weight vector:
X <- cbind(1, X1, X2)             # N x 3 design matrix
w <- matrix(runif(3, -1, 1) , 3, 1)    # 3 x 1 weight matrix
yhat <- sign(X %*% w)
# Iterate through the updating rule until classified perfectly:
while(sum(abs(yhat-Y) != 0))
{
wh_miss = which(abs(yhat-Y) != 0)
i.      = sample(wh_miss, 1)
w       = w + Y[i.]*X[i., ]
yhat    = sign(X %*% w)
}
return(list(weights = w, predictions = yhat))
}
PLA3 = function(X1, X2, X3, Y)
{
# Create a design matrix and weight vector:
X <- cbind(1, X1, X2, X3)             # N x 4 design matrix
w <- matrix(runif(4, -1, 1) , 4, 1)    # 4 x 1 weight matrix
yhat <- sign(X %*% w)
# Iterate through the updating rule until classified perfectly:
while(sum(abs(yhat-Y) != 0))
{
wh_miss = which(abs(yhat-Y) != 0)
i.      = sample(wh_miss, 1)
w       = w + Y[i.]*X[i., ]
yhat    = sign(X %*% w)
}
return(list(weights = w, predictions = yhat))
}
sim_number <- 100
N <- c(100, 200, 300, 400, 500)
two_d <- matrix(0, 100, 5)
three_d <- matrix(0, 100, 5)
for (j in 1:sim_number) {
for(i in 1:length(N)){
X1 <- runif(N[i], -1, 1)
X2 <- runif(N[i], -1, 1)
X <- cbind(1, X1, X2)
w <- c(0, -runif(1 , -1, 1), 1)
Y <- sign(X %*% w)
two_d[j,i] <- two_d_vec <- system.time(PLA2(X1, X2, Y))[[1]]
}
}
for (j in 1:sim_number) {
for(i in 1:length(N)){
X1 <- runif(N[i], -1, 1)
X2 <- runif(N[i], -1, 1)
X3 <- runif(N[i], -1, 1)
X <- cbind(1, X1, X2, X3)
w <- c(0, -runif(1 , -1, 1), 1, -runif(1 , -1, 1))
Y <- sign(X %*% w)
three_d[j, i] <- system.time(PLA3(X1, X2, X3, Y))[[1]]
}
}
mean_time_2d <- colMeans(two_d)
mean_time_3d <- colMeans(three_d)
plot(mean_time_2d ~ N, pch = 16, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.05))
lines(N, mean_time_3d, col = "red")
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"), lty = 1)
PLA2 = function(X1, X2, Y)
{
# Create a design matrix and weight vector:
X <- cbind(1, X1, X2)             # N x 3 design matrix
w <- matrix(runif(3, -1, 1) , 3, 1)    # 3 x 1 weight matrix
yhat <- sign(X %*% w)
# Iterate through the updating rule until classified perfectly:
while(sum(abs(yhat-Y) != 0))
{
wh_miss = which(abs(yhat-Y) != 0)
i.      = sample(wh_miss, 1)
w       = w + Y[i.]*X[i., ]
yhat    = sign(X %*% w)
}
return(list(weights = w, predictions = yhat))
}
PLA3 = function(X1, X2, X3, Y)
{
# Create a design matrix and weight vector:
X <- cbind(1, X1, X2, X3)             # N x 4 design matrix
w <- matrix(runif(4, -1, 1) , 4, 1)    # 4 x 1 weight matrix
yhat <- sign(X %*% w)
# Iterate through the updating rule until classified perfectly:
while(sum(abs(yhat-Y) != 0))
{
wh_miss = which(abs(yhat-Y) != 0)
i.      = sample(wh_miss, 1)
w       = w + Y[i.]*X[i., ]
yhat    = sign(X %*% w)
}
return(list(weights = w, predictions = yhat))
}
PLA2 = function(X1, X2, Y)
{
# Create a design matrix and weight vector:
X <- cbind(1, X1, X2)             # N x 3 design matrix
w <- matrix(runif(3, -1, 1) , 3, 1)    # 3 x 1 weight matrix
yhat <- sign(X %*% w)
# Iterate through the updating rule until classified perfectly:
while(sum(abs(yhat-Y) != 0))
{
wh_miss = which(abs(yhat-Y) != 0)
i.      = sample(wh_miss, 1)
w       = w + Y[i.]*X[i., ]
yhat    = sign(X %*% w)
}
return(list(weights = w, predictions = yhat))
}
PLA3 = function(X1, X2, X3, Y)
{
# Create a design matrix and weight vector:
X <- cbind(1, X1, X2, X3)             # N x 4 design matrix
w <- matrix(runif(4, -1, 1) , 4, 1)    # 4 x 1 weight matrix
yhat <- sign(X %*% w)
# Iterate through the updating rule until classified perfectly:
while(sum(abs(yhat-Y) != 0))
{
wh_miss = which(abs(yhat-Y) != 0)
i.      = sample(wh_miss, 1)
w       = w + Y[i.]*X[i., ]
yhat    = sign(X %*% w)
}
return(list(weights = w, predictions = yhat))
}
sim_number <- 100
N <- c(100, 200, 300, 400, 500)
two_d <- matrix(0, 100, 5)
three_d <- matrix(0, 100, 5)
for (j in 1:sim_number) {
for(i in 1:length(N)){
X1 <- runif(N[i], -1, 1)
X2 <- runif(N[i], -1, 1)
X <- cbind(1, X1, X2)
w <- c(0, -runif(1 , -1, 1), 1)
Y <- sign(X %*% w)
two_d[j,i] <- two_d_vec <- system.time(PLA2(X1, X2, Y))[[1]]
}
}
for (j in 1:sim_number) {
for(i in 1:length(N)){
X1 <- runif(N[i], -1, 1)
X2 <- runif(N[i], -1, 1)
X3 <- runif(N[i], -1, 1)
X <- cbind(1, X1, X2, X3)
w <- c(0, -runif(1 , -1, 1), 1, -runif(1 , -1, 1))
Y <- sign(X %*% w)
three_d[j, i] <- system.time(PLA3(X1, X2, X3, Y))[[1]]
}
}
mean_time_2d <- colMeans(two_d)
mean_time_3d <- colMeans(three_d)
plot(mean_time_2d ~ N, pch = 16, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.05))
lines(N, mean_time_3d, col = "red")
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"), lty = 1)
plot(mean_time_2d ~ N, pch = 16, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.06))
lines(N, mean_time_3d, col = "red")
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"), lty = 1)
plot(mean_time_2d ~ N, pch = 16, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.06))
lines(N, mean_time_3d, col = "red")
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"), lty = 1, cex = 2)
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"), lty = 1, cex = 1.5)
plot(mean_time_2d ~ N, pch = 16, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.06))
lines(N, mean_time_3d, col = "red")
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"), lty = 1, cex = 1.5)
plot(mean_time_2d ~ N, pch = 16, type = "l", col = "blue", xlab = "N", ylab = "Time Elapsed", ylim = c(0,0.06))
lines(N, mean_time_3d, col = "red")
legend("topleft", legend = c("Two Dimensional", "Three Dimensional"), col = c("Blue", "Red"), lty = 1, cex = 1.5)
data <- read.csv("rolling_average_all.csv")
>>>>>>> Stashed changes
data <- data[,-1]
#Eventually, add this into loop
#colnames(data)[c(10,11)] <- c("average_serve_rating", "average_return_rating")
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
predictive2 <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)%>%
select(c(1,57:98))
predictive3 <- predictive[seq(1,52522,2),-c(57:98)]
first_player<-predictive2[seq(1,52522,2),-1]
second_player<-predictive2[seq(2,52522,2),-1]
difference <- first_player-second_player
predictive_dataset <- cbind(predictive3,difference)
rm(predictive, predictive2, predictive3, second_player, first_player, difference)
colnames(predictive_dataset)[c(2,3)] <- c("Player_A", "Player_B")
predictive_dataset$wl <- ifelse(predictive_dataset$wl == 'winner', "Player A", "Player B")
predictive_dataset$wl <- as.factor(predictive_dataset$wl)
# Adding the seeding ---------------------------------------------------------
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
first_player<-predictive[seq(1,52522,2),-1]
player_1_seed <-first_player$seed
second_player<-predictive[seq(2,52522,2),-1]
player_2_seed <-second_player$seed
rm(predictive)
predictive_dataset$player_A_seed <- player_1_seed
predictive_dataset$player_B_seed <- player_2_seed
predictive_dataset <- predictive_dataset[,c(1:4,99:100,6:98)]
rm(first_player,second_player)
library(dplyr)
library(tidyverse)
library(lubridate)
library(magrittr)
library(caret)
library(stringr)
<<<<<<< Updated upstream
# Creating the predictive ----------------------------------------------------
=======
>>>>>>> Stashed changes
data <- read.csv("rolling_average_all.csv")
data <- data[,-1]
#Eventually, add this into loop
#colnames(data)[c(10,11)] <- c("average_serve_rating", "average_return_rating")
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
predictive2 <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)%>%
select(c(1,57:98))
predictive3 <- predictive[seq(1,52522,2),-c(57:98)]
first_player<-predictive2[seq(1,52522,2),-1]
second_player<-predictive2[seq(2,52522,2),-1]
difference <- first_player-second_player
predictive_dataset <- cbind(predictive3,difference)
rm(predictive, predictive2, predictive3, second_player, first_player, difference)
colnames(predictive_dataset)[c(2,3)] <- c("Player_A", "Player_B")
predictive_dataset$wl <- ifelse(predictive_dataset$wl == 'winner', "Player A", "Player B")
predictive_dataset$wl <- as.factor(predictive_dataset$wl)
# Adding the seeding ---------------------------------------------------------
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
first_player<-predictive[seq(1,52522,2),-1]
player_1_seed <-first_player$seed
second_player<-predictive[seq(2,52522,2),-1]
player_2_seed <-second_player$seed
rm(predictive)
predictive_dataset$player_A_seed <- player_1_seed
predictive_dataset$player_B_seed <- player_2_seed
predictive_dataset <- predictive_dataset[,c(1:4,99:100,6:98)]
rm(first_player,second_player)
<<<<<<< Updated upstream
#write.csv(predictive_dataset, file = "C:/Users/Adam Bresler/Documents/GitHub/Statistics-Honours-Project/Data/all_differences_no_custom_features.csv")
# Begin with feature design --------------------------------------------------
View(data)
View(predictive_dataset)
predictive_dataset2 <- predictive_dataset
View(predictive_dataset2)
View(predictive_dataset2)
library(gbm)
ctrl <- trainControl(method = 'cv', number = 5, verboseIter = T)
gbm_grid <- expand.grid(n.trees = c(250, 500, 1000),
interaction.depth = c(1, 2),
shrinkage = c(0.1, 0.05, 0.01),
n.minobsinnode = 1)
set.seed(2020)
View(predictive_dataset2)
predictive_dataset2$wl <- as.factor(predictive_dataset2$wl)
ind <- 1:23658
train_data <- predictive_dataset2[ind, ]
test_data <- predictive_dataset2[-ind, ]
library(gbm)
ctrl <- trainControl(method = 'cv', number = 5, verboseIter = T)
gbm_grid <- expand.grid(n.trees = c(250, 500, 1000),
interaction.depth = c(1, 2),
shrinkage = c(0.1, 0.05, 0.01),
n.minobsinnode = 1)
set.seed(2020)
gbm_tennis <- train(as.formula(paste(colnames(predictive_dataset2)[4], "~",
paste(colnames(data)[91:98], collapse = "+"),
sep = "")), data = train_data,
method = 'gbm',
distribution = 'bernoulli',
trControl = ctrl,
verbose = F,
tuneGrid = gbm_grid)
gbm_pred <- predict(gbm_tennis, test_data)
gbm_cf <- confusionMatrix(gbm_pred, test_data$wl)
sum(diag(gbm_cf$table))/sum(gbm_cf$table)
=======
setwd("~/GitHub/Statistics-Honours-Project/Data")
>>>>>>> Stashed changes
data <- read.csv("rolling_average_all.csv")
data <- data[,-1]
#Eventually, add this into loop
#colnames(data)[c(10,11)] <- c("average_serve_rating", "average_return_rating")
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
predictive2 <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)%>%
select(c(1,57:98))
predictive3 <- predictive[seq(1,52522,2),-c(57:98)]
first_player<-predictive2[seq(1,52522,2),-1]
second_player<-predictive2[seq(2,52522,2),-1]
difference <- first_player-second_player
predictive_dataset <- cbind(predictive3,difference)
rm(predictive, predictive2, predictive3, second_player, first_player, difference)
colnames(predictive_dataset)[c(2,3)] <- c("Player_A", "Player_B")
predictive_dataset$wl <- ifelse(predictive_dataset$wl == 'winner', "Player A", "Player B")
predictive_dataset$wl <- as.factor(predictive_dataset$wl)
# Adding the seeding ---------------------------------------------------------
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
first_player<-predictive[seq(1,52522,2),-1]
player_1_seed <-first_player$seed
second_player<-predictive[seq(2,52522,2),-1]
player_2_seed <-second_player$seed
rm(predictive)
predictive_dataset$player_A_seed <- player_1_seed
predictive_dataset$player_B_seed <- player_2_seed
predictive_dataset <- predictive_dataset[,c(1:4,99:100,6:98)]
rm(first_player,second_player)
<<<<<<< Updated upstream
predictive_dataset2 <- predictive_dataset
predictive_dataset2$wl <- as.factor(predictive_dataset2$wl)
ind <- 1:23658
train_data <- predictive_dataset2[ind, ]
test_data <- predictive_dataset2[-ind, ]
View(predictive_dataset2)
set.seed(2020)
mod <- glm(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(91:92,94:95)], collapse = "+"),
sep = "")), data = train_data, family = binomial)
threshold <- 0.3
y.hat <- ifelse(predict(mod, newdata = test_data, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test_data$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
threshold <- 0.5
y.hat <- ifelse(predict(mod, newdata = test_data, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test_data$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
threshold <- 0.45
y.hat <- ifelse(predict(mod, newdata = test_data, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test_data$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
plot(sort(predict(mod, type = 'response')), type = "l")
threshold <- 0.4
y.hat <- ifelse(predict(mod, newdata = test_data, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test_data$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
# Decision Tree --------------------------------------------------------------
library(tree)
#Super basic, default everything
set.seed(2020)
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[91:92,94:95)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(91:92,94:95)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
nrow(test_data)
#
rm(mod)
#
rm(tree_tennis)
str(predictive_dataset2$player_A_seed)
predictive_dataset2$seed <- predictive_dataset2$player_A_seed-predictive_dataset2$seed <- predictive_dataset2$player_B_seed
predictive_dataset2$seed <- predictive_dataset2$player_A_seed-predictive_dataset2$player_B_seed
View(predictive_dataset2)
#
rm(train_data,test_data)
#
rm(ind)
View(predictive_dataset2)
predictive_dataset2$wl <- as.factor(predictive_dataset2$wl)
predictive_dataset2$seed <- predictive_dataset2$player_A_seed-predictive_dataset2$player_B_seed
ind <- 1:23658
train_data <- predictive_dataset2[ind, ]
test_data <- predictive_dataset2[-ind, ]
#Super basic, default everything
set.seed(2020)
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(14,91:92,94:95,100)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
predictive_dataset2 <- predictive_dataset
predictive_dataset2$wl <- as.factor(predictive_dataset2$wl)
predictive_dataset2$seed <- predictive_dataset2$player_A_seed-predictive_dataset2$player_B_seed
ind <- 1:23658
train_data <- predictive_dataset2[ind, ]
test_data <- predictive_dataset2[-ind, ]
# Decision Tree --------------------------------------------------------------
library(tree)
#Super basic, default everything
set.seed(2020)
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(14,91:92,94:95,100)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(91:92,94:95,100)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(91:92,94:95)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(91:92,94:95,100)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
View(tree_tennis)
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
#Super basic, default everything
set.seed(2020)
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(14,100)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(100)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[100], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[c(91:92,94:95)], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
tree_tennis<- tree(wl~seed, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
View(predictive_dataset2)
View(predictive_dataset2)
tree_tennis<- tree(wl~seed+player_A_hand, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
set.seed(2020)
tree_tennis<- tree(wl~seed+player_A_hand+player_B_hand, data = train_data, split = 'deviance')
tree_tennis<- tree(wl~seed+surface, data = train_data, split = 'deviance')
View(predictive_dataset2)
set.seed(2020)
tree_tennis<- tree(wl~seed+tournament_surface, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
set.seed(2020)
tree_tennis<- tree(wl~seed+percent_first_serve_won.1, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
predictive_dataset2$percent_first_return_won.1
predictive_dataset2$percent_first_serve_return_won.1
set.seed(2020)
tree_tennis<- tree(wl~seed+percent_first_serve_won.1+percent_first_serve_return_won.1, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
set.seed(2020)
tree_tennis<- tree(wl~percent_first_serve_won.1+percent_first_serve_return_won.1, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
set.seed(2020)
tree_tennis<- tree(wl~seed, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
set.seed(2020)
tree_tennis<- tree(wl~seed+tournament_conditions, data = train_data, split = 'deviance')
set.seed(2020)
tree_tennis<- tree(wl~seed+as.factor(player_A_hand), data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
set.seed(2020)
tree_tennis<- tree(wl~seed+as.factor(tournament_surface), data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
set.seed(2020)
tree_tennis<- tree(wl~seed+as.factor(percent_total_points_won.1), data = train_data, split = 'deviance')
set.seed(2020)
tree_tennis<- tree(wl~seed+percent_total_points_won.1, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
set.seed(2020)
tree_tennis<- tree(wl~seed+percent_total_points_won.1+as.factor(tournament_surface), data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
set.seed(2020)
tree_tennis<- tree(wl~seed+percent_total_points_won.1+aces_percentage.1, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
set.seed(2020)
tree_tennis<- tree(wl~seed+percent_total_points_won.1+double_faults.1
, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
set.seed(2020)
tree_tennis<- tree(wl~seed+percent_total_points_won.1+aces.1
, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
set.seed(2020)
tree_tennis<- tree(wl~seed+percent_total_points_won.1+aces.1, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
=======
View(predictive_dataset)
write.csv(predictive_dataset, file = "C:/Users/Adam Bresler/Documents/GitHub/Statistics-Honours-Project/Data/all_differences_no_custom_features.csv")
library(dplyr)
library(tidyverse)
library(lubridate)
library(magrittr)
library(caret)
data <- read.csv("all_differences_no_custom_features.csv")
data <- read.csv("all_differences_no_custom_features.csv")
View(data)
data <- data[,-1]
ind <- 1:nrow(data)
View(data)
ind <- 1:23658
train <- predictive_dataset[ind, ]
test <- predictive_dataset[-ind, ]
train_data <- predictive_dataset[ind, ]
test_data <- predictive_dataset[-ind, ]
library(dplyr)
library(tidyverse)
library(lubridate)
library(magrittr)
library(caret)
data <- read.csv("all_differences_no_custom_features.csv")
data <- data[,-1]
# Model fitting --------------------------------------------------------------
ind <- 1:23658
train_data <- predictive_dataset[ind, ]
test_data <- predictive_dataset[-ind, ]
ind <- 1:23658
train_data <- data[ind, ]
test_data <- data[-ind, ]
colnames(mydata)[88:99]
colnames(data)[88:99]
set.seed(2020)
mod <- glm(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[88:99], collapse = "+"),
sep = "")), data = train_data, family = binomial)
summary(mod)
plot(sort(predict(mod, type = 'response')), type = "l")
data$wl <- as.factor(data$wl)
data <- read.csv("all_differences_no_custom_features.csv")
data <- data[,-1]
data$wl <- as.factor(data$wl)
ind <- 1:23658
train_data <- data[ind, ]
test_data <- data[-ind, ]
str(data)
set.seed(2020)
mod <- glm(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[88:99], collapse = "+"),
sep = "")), data = train_data, family = binomial)
summary(mod)
plot(sort(predict(mod, type = 'response')), type = "l")
threshold <- 0.3
y.hat <- ifelse(predict(mod, newdata = test, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
sens <- conf_matrix[2,2]/(conf_matrix[1,2]+conf_matrix[2,2])
spec <- conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[2,1])
threshold <- 0.3
y.hat <- ifelse(predict(mod, newdata = test_data, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
sens <- conf_matrix[2,2]/(conf_matrix[1,2]+conf_matrix[2,2])
spec <- conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[2,1])
>>>>>>> Stashed changes
library(dplyr)
library(tidyverse)
library(lubridate)
library(magrittr)
library(caret)
<<<<<<< Updated upstream
library(stringr)
# Creating the predictive ----------------------------------------------------
data <- read.csv("rolling_average_all.csv")
data <- data[,-1]
#Eventually, add this into loop
#colnames(data)[c(10,11)] <- c("average_serve_rating", "average_return_rating")
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
predictive2 <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)%>%
select(c(1,57:98))
predictive3 <- predictive[seq(1,52522,2),-c(57:98)]
first_player<-predictive2[seq(1,52522,2),-1]
second_player<-predictive2[seq(2,52522,2),-1]
difference <- first_player-second_player
predictive_dataset <- cbind(predictive3,difference)
rm(predictive, predictive2, predictive3, second_player, first_player, difference)
colnames(predictive_dataset)[c(2,3)] <- c("Player_A", "Player_B")
predictive_dataset$wl <- ifelse(predictive_dataset$wl == 'winner', "Player A", "Player B")
predictive_dataset$wl <- as.factor(predictive_dataset$wl)
# Adding the seeding ---------------------------------------------------------
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
first_player<-predictive[seq(1,52522,2),-1]
player_1_seed <-first_player$seed
second_player<-predictive[seq(2,52522,2),-1]
player_2_seed <-second_player$seed
rm(predictive)
predictive_dataset$player_A_seed <- player_1_seed
predictive_dataset$player_B_seed <- player_2_seed
predictive_dataset <- predictive_dataset[,c(1:4,99:100,6:98)]
rm(first_player,second_player)
predictive_dataset2 <- predictive_dataset
predictive_dataset2$wl <- as.factor(predictive_dataset2$wl)
predictive_dataset2$seed <- predictive_dataset2$player_A_seed-predictive_dataset2$player_B_seed
ind <- 1:23658
train_data <- predictive_dataset2[ind, ]
test_data <- predictive_dataset2[-ind, ]
# Decision Tree --------------------------------------------------------------
library(tree)
predictive_dataset2$double_faults.1
set.seed(2020)
tree_tennis<- tree(wl~seed+percent_total_points_won.1+aces.1, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
tree_tennis<- tree(wl~seed+percent_total_points_won.1+aces.1, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
tree_tennis<- tree(wl~seed+percent_total_points_won.1+double_faults.1, data = train_data, split = 'deviance')
=======
data <- read.csv("all_differences_no_custom_features.csv")
data <- data[,-1]
data$wl <- as.factor(data$wl)
# Model fitting --------------------------------------------------------------
ind <- 1:23658
train_data <- data[ind, ]
test_data <- data[-ind, ]
set.seed(2020)
mod <- glm(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[88:99], collapse = "+"),
sep = "")), data = train_data, family = binomial)
summary(mod)
plot(sort(predict(mod, type = 'response')), type = "l")
threshold <- 0.3
y.hat <- ifelse(predict(mod, newdata = test_data, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test_data$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
sens <- conf_matrix[2,2]/(conf_matrix[1,2]+conf_matrix[2,2])
spec <- conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[2,1])
threshold <- 0.5
y.hat <- ifelse(predict(mod, newdata = test_data, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test_data$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
sens <- conf_matrix[2,2]/(conf_matrix[1,2]+conf_matrix[2,2])
threshold <- 0.4
y.hat <- ifelse(predict(mod, newdata = test_data, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test_data$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
sens <- conf_matrix[2,2]/(conf_matrix[1,2]+conf_matrix[2,2])
spec <- conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[2,1])
threshold <- 0.3
y.hat <- ifelse(predict(mod, newdata = test_data, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test_data$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
sens <- conf_matrix[2,2]/(conf_matrix[1,2]+conf_matrix[2,2])
spec <- conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[2,1])
set.seed(2020)
tree_tennis<- tree(wl ~ average_serve_rating + average_return_rating, data = train_data, split = 'deviance')
summary(tree_tennis)
tree_tennis
plot(tree_tennis)
text(tree_tennis, cex = 0.9)
>>>>>>> Stashed changes
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
<<<<<<< Updated upstream
tree_tennis<- tree(wl~seed+percent_total_points_won.1, split = 'deviance')
tree_tennis<- tree(wl~seed+percent_total_points_won.1, data = train_data, split = 'deviance')
=======
# Decision Tree --------------------------------------------------------------
library(tree)
set.seed(2020)
tree_tennis<- tree(wl ~ average_serve_rating + average_return_rating, data = train_data, split = 'deviance')
summary(tree_tennis)
tree_tennis
plot(tree_tennis)
text(tree_tennis, cex = 0.9)
>>>>>>> Stashed changes
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
<<<<<<< Updated upstream
tree_tennis<- tree(wl~seed, data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
tree_tennis<- tree(wl~as.factor(player_A_hand), data = train_data, split = 'deviance')
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
tree_tennis<- tree(wl~seed, data = train_data, split = 'deviance')
=======
library(tree)
#Super basic, default everything
set.seed(2020)
tree_tennis<- tree(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[88:99], collapse = "+"),
sep = "")), data = train_data, split = 'deviance')
summary(tree_tennis)
tree_tennis
plot(tree_tennis)
text(tree_tennis, cex = 0.9)
>>>>>>> Stashed changes
yhat<- predict(tree_tennis,  test_data, type = 'class')
(c_mat <- table(yhat, test_data$wl))
sum(diag(c_mat))/nrow(test_data)*100
1 - sum(diag(c_mat))/nrow(test_data)
<<<<<<< Updated upstream
=======
library(randomForest)
set.seed(2020)
rf_tennis <- randomForest(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[88:99], collapse = "+"),
sep = "")), data = train_data,
ntree = 1000, #no mtry argument, keep it defualt
importance = TRUE,
do.trace = 100)
rf_tennis
plot(rf_tennis$err.rate[, 'OOB'], type = 's', xlab = 'Number of trees', ylab = 'OOB error')
rf_pred <- predict(rf_tennis, newdata = test_data)
table(rf_pred, test_data$wl)
(rf_err <- mean(rf_pred != test_data$wl))
library(gbm)
ctrl <- train_dataControl(method = 'cv', number = 5, verboseIter = T)
gbm_grid <- expand.grid(n.trees = c(250, 500, 1000),
interaction.depth = c(1, 2),
shrinkage = c(0.1, 0.05, 0.01),
n.minobsinnode = 1)
set.seed(2020)
gbm_tennis <- train_data(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[88:99], collapse = "+"),
sep = "")), data = train_data,
method = 'gbm',
distribution = 'bernoulli',
trControl = ctrl,
verbose = F,
tuneGrid = gbm_grid)
gbm_pred <- predict(gbm_tennis, test_data)
gbm_cf <- confusionMatrix(gbm_pred, test_data$wl)
sum(diag(gbm_cf$table))/sum(gbm_cf$table)
library(gbm)
ctrl <- train_dataControl(method = 'cv', number = 5, verboseIter = T)
gbm_grid <- expand.grid(n.trees = c(250, 500, 1000),
interaction.depth = c(1, 2),
shrinkage = c(0.1, 0.05, 0.01),
n.minobsinnode = 1)
set.seed(2020)
gbm_tennis <- train(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[88:99], collapse = "+"),
sep = "")), data = train_data,
method = 'gbm',
distribution = 'bernoulli',
trControl = ctrl,
verbose = F,
tuneGrid = gbm_grid)
gbm_pred <- predict(gbm_tennis, test_data)
gbm_cf <- confusionMatrix(gbm_pred, test_data$wl)
sum(diag(gbm_cf$table))/sum(gbm_cf$table)
library(gbm)
ctrl <- trainControl(method = 'cv', number = 5, verboseIter = T)
gbm_grid <- expand.grid(n.trees = c(250, 500, 1000),
interaction.depth = c(1, 2),
shrinkage = c(0.1, 0.05, 0.01),
n.minobsinnode = 1)
set.seed(2020)
gbm_tennis <- train(as.formula(paste(colnames(data)[4], "~",
paste(colnames(data)[88:99], collapse = "+"),
sep = "")), data = train_data,
method = 'gbm',
distribution = 'bernoulli',
trControl = ctrl,
verbose = F,
tuneGrid = gbm_grid)
gbm_pred <- predict(gbm_tennis, test_data)
gbm_cf <- confusionMatrix(gbm_pred, test_data$wl)
sum(diag(gbm_cf$table))/sum(gbm_cf$table)
>>>>>>> Stashed changes
