final_data <- read.csv('data_no_ytd.csv')
final_data <- final_data[, -1]
final_data <- read.csv('data_no_ytd.csv')
library(dplyr)
library(tidyverse)
library(lubridate)
library(magrittr)
data <- read.csv("rolling_average_serve_return.csv")
data <- data[,-1]
#Eventually, add this into loop
colnames(data)[c(10,11)] <- c("average_serve_rating", "average_return_rating")
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
predictive2 <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)%>%
select(c(1,10,11))
predictive3 <- predictive[seq(1,52668,2),-c(10,11)]
first_player<-predictive2[seq(1,52668,2),-1]
second_player<-predictive2[seq(2,52668,2),-1]
difference <- first_player-second_player
predictive_dataset <- cbind(predictive3,difference)
rm(predictive, predictive2, predictive3, second_player, first_player, difference)
colnames(predictive_dataset)[c(2,3)] <- c("Player_A", "Player_B")
predictive_dataset$wl <- ifelse(predictive_dataset$wl == 'winner', "Player A", "Player B")
predictive_dataset$wl <- as.factor(predictive_dataset$wl)
# Model fitting --------------------------------------------------------------
ind <- 1:23731
train <- predictive_dataset[ind, ]
test <- predictive_dataset[-ind, ]
set.seed(2020)
mod <- glm(wl ~ average_serve_rating + average_return_rating, data = train, family = binomial)
summary(mod)
plot(sort(predict(mod, type = 'response')), type = "l")
threshold <- 0.3
y.hat <- ifelse(predict(mod, newdata = test, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
sens <- conf_matrix[2,2]/(conf_matrix[1,2]+conf_matrix[2,2])
spec <- conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[2,1])
# Decision Tree --------------------------------------------------------------
library(tree)
#Super basic, default everything
set.seed(2020)
tree_tennis<- tree(wl ~ average_serve_rating + average_return_rating, data = train, split = 'deviance')
summary(tree_tennis)
tree_tennis
plot(tree_tennis)
text(tree_tennis, cex = 0.9)
yhat<- predict(tree_tennis,  test, type = 'class')
(c_mat <- table(yhat, test$wl))
sum(diag(c_mat))/nrow(test)*100
1 - sum(diag(c_mat))/nrow(test)
# Random Forest --------------------------------------------------------------
#Super basic, default everything
library(randomForest)
set.seed(2020)
rf_tennis <- randomForest(wl ~ average_serve_rating + average_return_rating, data = train,
ntree = 1000, #no mtry argument, keep it defualt
importance = TRUE,
do.trace = 100)
rf_tennis
plot(rf_tennis$err.rate[, 'OOB'], type = 's', xlab = 'Number of trees', ylab = 'OOB error')
rf_pred <- predict(rf_tennis, newdata = test)
table(rf_pred, test$wl)
(rf_err <- mean(rf_pred != test$wl))
library(dplyr)
library(tidyverse)
library(lubridate)
library(magrittr)
setwd("~/GitHub/Statistics-Honours-Project/Data")
data <- read.csv("rolling_average_serve_return.csv")
data <- data[,-1]
#Eventually, add this into loop
colnames(data)[c(10,11)] <- c("average_serve_rating", "average_return_rating")
predictive <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)
predictive2 <- data %>%
group_by(Match_ID)%>%arrange(.by_group = TRUE)%>%
select(c(1,10,11))
predictive3 <- predictive[seq(1,52668,2),-c(10,11)]
first_player<-predictive2[seq(1,52668,2),-1]
second_player<-predictive2[seq(2,52668,2),-1]
difference <- first_player-second_player
predictive_dataset <- cbind(predictive3,difference)
rm(predictive, predictive2, predictive3, second_player, first_player, difference)
colnames(predictive_dataset)[c(2,3)] <- c("Player_A", "Player_B")
predictive_dataset$wl <- ifelse(predictive_dataset$wl == 'winner', "Player A", "Player B")
predictive_dataset$wl <- as.factor(predictive_dataset$wl)
# Model fitting --------------------------------------------------------------
ind <- 1:23731
train <- predictive_dataset[ind, ]
test <- predictive_dataset[-ind, ]
set.seed(2020)
mod <- glm(wl ~ average_serve_rating + average_return_rating, data = train, family = binomial)
summary(mod)
plot(sort(predict(mod, type = 'response')), type = "l")
threshold <- 0.3
y.hat <- ifelse(predict(mod, newdata = test, type = 'response') > threshold, "Player A", "Player B")
y.hat[which(is.na(y.hat))]
conf_matrix <- table(y.hat, test$wl)
conf_matrix
sum(diag(conf_matrix))/sum(conf_matrix)
sens <- conf_matrix[2,2]/(conf_matrix[1,2]+conf_matrix[2,2])
spec <- conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[2,1])
# Decision Tree --------------------------------------------------------------
library(tree)
#Super basic, default everything
set.seed(2020)
tree_tennis<- tree(wl ~ average_serve_rating + average_return_rating, data = train, split = 'deviance')
summary(tree_tennis)
tree_tennis
plot(tree_tennis)
text(tree_tennis, cex = 0.9)
yhat<- predict(tree_tennis,  test, type = 'class')
(c_mat <- table(yhat, test$wl))
sum(diag(c_mat))/nrow(test)*100
1 - sum(diag(c_mat))/nrow(test)
# Random Forest --------------------------------------------------------------
#Super basic, default everything
library(randomForest)
set.seed(2020)
rf_tennis <- randomForest(wl ~ average_serve_rating + average_return_rating, data = train,
ntree = 1000, #no mtry argument, keep it defualt
importance = TRUE,
do.trace = 100)
rf_tennis
plot(rf_tennis$err.rate[, 'OOB'], type = 's', xlab = 'Number of trees', ylab = 'OOB error')
rf_pred <- predict(rf_tennis, newdata = test)
table(rf_pred, test$wl)
(rf_err <- mean(rf_pred != test$wl))
# Boosting -------------------------------------------------------------------
library(gbm)
train$wl <- as.numeric(train$wl) - 1
str(train)
set.seed(2020)
gbm_lib <- gbm(wl ~ average_serve_rating + average_return_rating, data = train,
distribution = 'bernoulli',
n.trees = 1000, #B
interaction.depth = 2, #d
shrinkage = 0.01, #lambda
bag.fraction = 1,
cv.folds = 10, #built-in CV
verbose = F)
yhat_gbm <- predict.gbm(gbm_lib, test)
(mse_gbm <- mean((test$wl - yhat_gbm)^2))
View(gbm_lib)
View(train)
